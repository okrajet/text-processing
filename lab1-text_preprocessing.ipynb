{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f894a4c1-9f5b-4011-a03d-ea963322766d",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b91fcf-e4a9-4f5d-9f4d-95ec31e00ebc",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple text preprocessing pipeline using the [Natural Language Toolkit (NLTK)](https://www.nltk.org/index.html). \n",
    "\n",
    "Make sure you first follow the [instructions on Wattle](https://wattlecourses.anu.edu.au/mod/page/view.php?id=2943340) to set up your environment for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cae73d-a4a7-48b2-af42-f39310bb9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1a746-d6a2-4fdd-be1e-969fd31a0353",
   "metadata": {},
   "source": [
    "Raw text from [this Wikipedia page](https://en.wikipedia.org/wiki/Australia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5bd45-0578-450a-bfab-3397b710cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. With an area of 7,617,930 square kilometres (2,941,300 sq mi), Australia is the largest country by area in Oceania and the world's sixth-largest country. Australia is the oldest, flattest, and driest inhabited continent, with the least fertile soils. It is a megadiverse country, and its size gives it a wide variety of landscapes and climates, with deserts in the centre, tropical rainforests in the north-east, and mountain ranges in the south-east.\\nIndigenous Australians have inhabited the continent for approximately 65,000 years. The European maritime exploration of Australia commenced in the early 17th century with the arrival of Dutch explorers. In 1770, Australia's eastern half was claimed by Great Britain and initially settled through penal transportation to the colony of New South Wales from 26 January 1788, a date which became Australia's national day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08b4e1-d68b-4665-bf38-5f3930242c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f2b2a-bf96-4ac3-86fb-b976aa706a62",
   "metadata": {},
   "source": [
    "## Sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171d2d2-35fc-4061-8c61-d894c73aae9a",
   "metadata": {},
   "source": [
    "Splitting text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e6f49-4d2d-4a64-85f3-99c6a4d5e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78956490-64c6-42dc-8de3-21b050e3da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the below line to see the documentation of `sent_tokenize'\n",
    "# sent_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414b145-56b2-437d-aa71-ce41432391ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08169b36-63ac-49b2-a196-8cfb2137e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3602844-7644-4f34-b963-307ca9536a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(sentences)} sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66bc6a-cd25-4550-a61e-35a6a1dd5342",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede3f5c-3d38-43ee-b0bc-2180b39dadd4",
   "metadata": {},
   "source": [
    "Dividing a string into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598eb45-0690-4da1-acdc-06c0153e5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae93eb-ab29-49c1-9df6-25f1c433bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bd16b-636e-41a7-b510-374d041c7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [word_tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c636272-04f5-4d05-9dd9-4915570f1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a093f-f14d-4629-b64c-ed193f2ba349",
   "metadata": {},
   "source": [
    "The top-10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc7652-015c-4598-ac19-fc95a5b9b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([w for x in tokens_list for w in x]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3800eb4-a4ff-4334-b40c-7f710abb5002",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try [other tokenisers provided by NLTK](https://www.nltk.org/api/nltk.tokenize.html) (e.g. RegexpTokenizer, WhitespaceTokenizer, WordPunctTokenizer etc.) and compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3c1a-31ef-44f6-95ed-b4825f65a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8392f0a-a6d6-40b3-a216-b2e24e00c4b9",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "What are the differences and how can we choose the best tokeniser for a task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c070b-ae72-46aa-a6b5-580aac52ffed",
   "metadata": {},
   "source": [
    "## Removing punctuation and stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649191b-f526-4862-9ad9-adc3cf9e0f4e",
   "metadata": {},
   "source": [
    "Stopwords and punctuation are usually not helpful for many IR tasks, and removing them can reduce the number of tokens we need to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509dc6f-28f6-4de3-a729-30fb5871741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc9103-97b8-430f-864c-9868bcdb0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4383770-de61-40b7-96f2-c5670a971cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13a8a0-6d24-4771-b28f-ef5e7f03e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa8b29-38a4-45f6-a1e8-d73fd1eea005",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list[:] = [[w for w in x if w not in string.punctuation and w not in stopwords_en] for x in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0e0e9-11ef-44a8-ad48-d03a9b578bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b249d-d117-4ab1-b95a-ae2412b033d5",
   "metadata": {},
   "source": [
    "The top-10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7ed35-78b5-4a31-a159-a0593706ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([w for x in tokens_list for w in x]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a02d0a-20a0-4950-98fc-0ae5184686d2",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Will we get a different set of tokens if we lower casing all words before removing stopwords? What are the potential problems by doing that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95872ec7-9e27-43ab-bd8e-37d71ca58433",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8eeda-cd4d-469f-a272-62a6b04cb68e",
   "metadata": {},
   "source": [
    "Turning words into stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778dfba-d3d8-405e-84ad-82a74b33ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec654e-ed6d-4da4-b5ac-4174687bd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40f01f-e4db-407c-a769-40a8c5f61d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stem = [stemmer.stem(w) for x in tokens_list for w in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cab22-066c-40c7-ab23-42f729480d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d9d58-6a85-475f-8036-d0fc712a09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens_stem).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5750de-a41d-4ea2-be84-e4ced36bfb1a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try other NLTK stemmers (e.g. SnowballStemmer, RegexpStemmer), you may need to download additional data packages, see https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17a2ba-750f-4a6f-99de-c00bdaca90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import SnowballStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00477ff9-4273-4696-98c4-b5a6a4f39911",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83edc5-0cae-437e-8528-9e5e2169e46a",
   "metadata": {},
   "source": [
    "Turning words into lemmas (entries in a dictionary). It requires knowledge of the context (typically the intended\n",
    "Part-of-Speech of a word in the context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e611d-8d23-4913-b60c-0160bdbf4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e9018-4770-4eac-bfaf-ec6f8fbbf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869121b8-512a-406f-a87c-21e47ded0321",
   "metadata": {},
   "source": [
    "POS tagging for lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e9325-251b-47b2-9833-0e13f7d6f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tags_list = nltk.pos_tag_sents(tokens_list)\n",
    "# tags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baddbec-df26-497f-9dde-1adddb649d7f",
   "metadata": {},
   "source": [
    "A heuristic to convert POS tags to the [four syntactic categories that wordnet recognizes (i.e. **noun**, **verb**, **adj** and **adv**)](https://wordnet.princeton.edu/):\n",
    "- `n` for nouns\n",
    "- `v` for verbs\n",
    "- `a` for adjectives\n",
    "- `r` for adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d42ac-4867-42f3-8ef7-cbf98740f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05f26f-a383-444e-88e3-e885b91d39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_tag = lambda t: 'a' if t == 'j' else (t if t in ['n', 'v', 'r'] else 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ff542-80e2-44d9-9ff2-4ce3f991b525",
   "metadata": {},
   "source": [
    "Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df130d-6918-4ca6-a0c2-b859f75843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe6599-de52-4abd-8a2c-6edfc2ce729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lemma = [lemmatizer.lemmatize(w.lower(), pos=wordnet_tag(t[0].lower())) for x in tags_list for (w, t) in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc312e00-2a09-46a4-8b3e-6cd15766a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e5ea-1537-4def-a5c3-506b2e540e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens_lemma).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869e3c1-e510-4166-be69-34a212dc20b4",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compare the results of stemming and lemmatisation. Can you see the differences and the potential problems with stemming and lemmatisation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
